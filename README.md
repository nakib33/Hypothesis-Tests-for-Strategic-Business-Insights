# ğŸš€ Hypothesis Tests for Strategic Business Insights


---

## ğŸ“Œ Description

---

This repository contains 10 comprehensive projects focused on applying various statistical hypothesis tests to generate strategic business insights. Each project uses synthetic or real datasets to demonstrate how hypothesis testing can aid in making informed business decisions by validating assumptions, comparing groups, and identifying relationships between variables. The projects span parametric and non-parametric tests to handle different data characteristics and assumptions, providing a robust toolkit for data-driven analysis.

## ğŸ“ Table of Contents
- [Description](#description)
- [Projects Overview](#projects-overview)
- [Detailed Project Information](#detailed-project-information)
  - [1. A/B Testing](#1-ab-testing)
  - [2. Chi-Square Test](#2-chi-square-test)
  - [3. Two-Sample T-Test](#3-two-sample-t-test)
  - [4. Paired T-Test](#4-paired-t-test)
  - [5. ANOVA](#5-anova)
  - [6. Correlation Coefficient](#6-correlation-coefficient)
  - [7. Mann-Whitney U Test](#7-mann-whitney-u-test)
  - [8. Wilcoxon Signed-Rank Test](#8-wilcoxon-signed-rank-test)
  - [9. Kruskal-Wallis Test](#9-kruskal-wallis-test)
  - [10. Chi-Square Goodness of Fit Test](#10-chi-square-goodness-of-fit-test)
- [Technical Implementation](#technical-implementation)
- [How to Run These Projects](#how-to-run-these-projects)
- [Conclusion](#conclusion)



---

## Projects Overview

| Project No. | Project Title                                               |
|-------------|-------------------------------------------------------------|
| 1           | A/B Testing                                                 |
| 2           | Chi-Square Test                                            |
| 3           | Two-Sample T-Test                                          |
| 4           | Paired T-Test                                              |
| 5           | ANOVA                                                      |
| 6           | Correlation Coefficient                                    |
| 7           | Mann-Whitney U Test (Non-parametric alternative to Two-Sample t-test) |
| 8           | Wilcoxon Signed-Rank Test (Non-parametric paired test)    |
| 9           | Kruskal-Wallis Test (Non-parametric ANOVA)                |
| 10          | Chi-Square Goodness of Fit Test                            |

---

## Detailed Project Information

## 1. A/B Testing
ğŸ“„ **Project Link:** **[A_B_Testing_Project.ipynb](Hypothesis%20Testing/AB%20Testing/A_B_Testing_Project.ipynb)**

## A/B Testing Project: Evaluating Button Color Impact on User Conversions  

### ğŸ“Œ Overview  
This project conducts a rigorous A/B testing analysis to evaluate the impact of two different button color variants (Blue vs. Green) on user conversion rates for an e-commerce platform. Using a real-world dataset from Kaggle, the study follows a structured approach, including:  
- **Exploratory Data Analysis (EDA)**  
- **Assumption Verification** (Normality & Homogeneity of Variances)  
- **Statistical Testing** (Independent Samples t-test & Mann-Whitney U Test)  
- **Effect Size & Power Analysis**  
- **Result Interpretation & Business Recommendations**  

ğŸ”— **Dataset Source**: [A/B Testing for Button Color Variants Dataset (Kaggle)](https://www.kaggle.com/datasets/example/ab-testing-button-color)  


### â“ Problem Statement  
E-commerce platforms continuously experiment with UI elements (like button colors) to maximize user engagement and conversions. However, without proper statistical validation, design changes may not yield meaningful improvements.  

**Key Question**:  
*Does changing the button color (from Blue to Green) lead to a statistically significant increase in user conversion rates?*  


### ğŸ¯ Objective  
- **Determine** if there is a statistically significant difference in conversion rates between the two button colors.  
- **Assess** the practical significance of the change using effect size and power analysis.  
- **Provide** data-driven recommendations for UI optimization.  


### ğŸ“Š Dataset Description  
The dataset contains synthetic user interaction data simulating an A/B test with two button variants:  
- **Control Group (A)**: Blue button  
- **Treatment Group (B)**: Green button  

**Key Features**:  
- `user_id`: Unique identifier for each user  
- `group`: Indicates whether the user was in Group A (Control) or Group B (Treatment)  
- `clicked`: Binary indicator (1 = clicked, 0 = did not click)  
- `converted`: Binary indicator (1 = converted, 0 = did not convert)  

**Size**: <1MB (Ideal for quick analysis)  


### ğŸš€ Project Highlights  
âœ… **Comprehensive EDA**: Visualized conversion rates, checked distributions (histograms, Q-Q plots), and compared engagement metrics.  
âœ… **Assumption Checks**: Verified normality (Shapiro-Wilk) and homogeneity of variances (Leveneâ€™s test).  
âœ… **Statistical Testing**:  
   - **Independent Samples t-test** (for normally distributed data)  
   - **Mann-Whitney U Test** (non-parametric alternative)  
âœ… **Effect Size & Power Analysis**: Calculated Cohenâ€™s d (effect size) and statistical power to assess practical significance.  
âœ… **Actionable Insights**: Provided clear recommendations based on statistical evidence.  


### ğŸ” Why This Project?  
- Demonstrates **real-world A/B testing methodology** from hypothesis formulation to result interpretation.  
- Highlights **importance of statistical validation** before implementing UI changes.  
- Provides a **template for data-driven decision-making** in marketing & UX optimization.  


### ğŸ’¡ Benefits / Use Cases  
ğŸ“Œ **For Businesses**:  
- Helps optimize UI elements to **increase conversions & revenue**.  
- Reduces guesswork by relying on **statistically validated insights**.  

ğŸ“Œ **For Data Scientists**:  
- Serves as a **reference for A/B testing best practices**.  
- Demonstrates **assumption checks, test selection, and interpretation**.  

ğŸ“Œ **For Product Teams**:  
- Guides **data-backed design decisions** for better user engagement.  


### ğŸ“ˆ Statistical Tests Used  
1. **Normality Check**: Shapiro-Wilk Test  
2. **Equal Variance Check**: Leveneâ€™s Test  
3. **Hypothesis Testing**:  
   - **Independent Samples t-test** (Parametric)  
   - **Mann-Whitney U Test** (Non-parametric)  
4. **Effect Size**: Cohenâ€™s d  
5. **Power Analysis**: Statistical Power (1 - Î²)  

### ğŸ”‘ Key Findings & Result Summary  
### Hypothesis Testing Results  
- **p-value (t-test)**: **0.927** (Fail to reject Hâ‚€)  
- **p-value (Mann-Whitney U)**: **0.928** (Fail to reject Hâ‚€)  
- **Conclusion**: **No statistically significant difference** in conversion rates between Blue & Green buttons.  

### Effect Size & Power  
- **Cohenâ€™s d**: **0.500** (Medium effect size)  
- **Statistical Power**: **0.85** (Adequate to detect an effect)  

### Final Recommendation  
ğŸ“¢ **"No significant improvement in conversions was observed when changing the button color from Blue to Green. Thus, retaining the original Blue button is recommended unless further tests with different variations suggest otherwise."**


---

## 2. Chi-Square Test

## **Chi-Square Test Project: Gender vs Purchase Likelihood Analysis**  

ğŸ“„ **Project Link:** **[Chi_Square_Testing.ipynb](Hypothesis%20Testing/Chi-Square%20Test/Chi_Square_Testing.ipynb)**

### ğŸ“Œ Overview  
This project employs the **Chi-Square Test of Independence** to investigate whether a statistically significant association exists between customer gender and product purchase likelihood in an e-commerce context. Using a carefully crafted synthetic dataset that mimics real-world shopping behavior, the analysis includes:  
- Data visualization of key variables (gender, purchase status, age)  
- Contingency table construction  
- Chi-Square test assumption verification  
- Statistical testing and interpretation  
- Actionable business insights  


###  â“ Problem Statement  
E-commerce businesses often wonder if demographic factors like gender influence purchasing decisions. This analysis answers:  
**"Does customer gender significantly affect the likelihood of purchasing our product?"**  


###  ğŸ¯ Objective  
- Determine if gender and purchase likelihood are statistically associated  
- Validate assumptions for Chi-Square testing  
- Provide data-driven recommendations for marketing strategies  


###  ğŸ“Š Dataset Description  
**Synthetic Dataset** (500 records) simulating real customer behavior:  
- `Gender`: Binary (Male/Female) with near-equal distribution  
- `Purchase`: Binary (Yes/No) with gender-biased probabilities  
- `Age`: Normally distributed (Î¼=35, Ïƒ=10), clipped to 18-70 years  

**Key Characteristics**:  
âœ… Balanced gender distribution (Male: 233, Female: 267)  
âœ… Realistic purchase probability variation (Male: 45%, Female: 55%)  
âœ… Numeric age variable for additional demographic analysis  


###  ğŸš€ Project Highlights  
âœ… **Comprehensive EDA**: Countplots, histograms, and contingency tables  
âœ… **Assumption Verification**:  
   - Independence of observations  
   - Expected frequencies â‰¥5 (all cells satisfied)  
âœ… **Statistical Testing**:  
   - Chi-Square Test of Independence  
   - Effect size calculation (Cramer's V optional)  
âœ… **Clear Interpretation**: Business-focused conclusions  


###  ğŸ” Why This Project?  
- Demonstrates **real-world application** of Chi-Square testing  
- Highlights **importance of assumption checks** in statistical analysis  
- Provides **template for customer segmentation studies**  


###  ğŸ’¡ Benefits / Use Cases  
ğŸ“Œ **For Marketing Teams**:  
- Avoids gender-based assumptions in campaigns  
- Guides resource allocation for promotions  

ğŸ“Œ **For Data Scientists**:  
- Complete workflow from synthetic data generation to interpretation  
- Best practices for categorical data analysis  

ğŸ“Œ **For Product Managers**:  
- Evidence-based decisions on product positioning  


###  ğŸ“ˆ Statistical Tests Used  
1. **Chi-Square Test of Independence**  
   - Test Statistic: Ï‡Â² = 1.9395  
   - Degrees of Freedom: 1  
   - p-value: 0.1641  
2. **Expected Frequency Verification**  
3. **Effect Size Measurement** (Optional: Cramer's V/Phi Coefficient)  


### **ğŸ”‘ Key Findings & Result Summary**  
**Hypothesis Testing Results:**  
- **p-value**: 0.1641 (> 0.05 significance level)  
- **Conclusion**: Fail to reject null hypothesis  

**Business Interpretation:** 
ğŸ“¢ **"No statistically significant association found between gender and purchase likelihood. Marketing strategies should not prioritize gender-based targeting for this product."**  

### Supplementary Insights  
- Observed purchase rates:  
  - Female: 55%  
  - Male: 45%  
- While numeric differences exist, they're **not statistically significant**
  
---

## 3. Two-Sample T-Test

## ğŸ“Š **Two-Sample T-Test: Revenue Comparison Between North and South Regions**  

ğŸ“„ **Project Link:** **[Two-Sample T-Test.ipynb](Hypothesis%20Testing/Hypothesis%20Tests/1.%20Two-Sample%20T-Test.ipynb)**

### ğŸŒ Overview  
This project performs a Two-Sample Independent t-test to analyze whether there's a statistically significant difference in average revenue between North and South regions of a retail business. Using a carefully generated synthetic dataset, we:  
â€¢ Visualize revenue distributions  
â€¢ Validate statistical assumptions  
â€¢ Conduct hypothesis testing  
â€¢ Derive business insights  

### â“ Problem Statement  
A retail chain observes slight revenue differences between its North and South locations. Management needs to know:  
*"Is this observed difference statistically significant, or just random variation?"*  

### ğŸ¯ Objective  
â€¢ Determine if regional revenue differences are statistically meaningful  
â€¢ Demonstrate proper t-test application with assumption checks  
â€¢ Provide data-backed recommendations for regional strategy  

### ğŸ“Š Dataset Description  
**Synthetic Revenue Data (500 records):**  
â€¢ **North Region (250 samples):**  
  - Mean: $50,000  
  - Standard Deviation: $8,000  
â€¢ **South Region (250 samples):**  
  - Mean: $51,000  
  - Standard Deviation: $8,500  

**Variables:**  
â€¢ `region`: Categorical (North/South)  
â€¢ `revenue`: Continuous numerical values  

### ğŸš€ Project Highlights  
âœ… **Data Visualization:**  
   - Histograms with KDE plots  
   - Q-Q plots for normality inspection  

âœ… **Rigorous Assumption Testing:**  
   - Shapiro-Wilk normality test (North p=0.126, South p=0.884)  
   - Levene's test for equal variances  

âœ… **Proper Statistical Testing:**  
   - Independent Two-Sample t-test implementation  
   - Correct interpretation of p-values  

âœ… **Business-Ready Conclusions:**  
   - Clear statistical findings translated to actionable insights  

### ğŸ” Why This Project Matters  
â€¢ Real-world demonstration of t-test methodology  
â€¢ Emphasizes critical assumption validation step  
â€¢ Provides template for regional performance analysis  

### ğŸ’¡ Business Value  
ğŸ“Œ **For Executives:**  
   - Evidence-based regional strategy decisions  
   - Avoids overreacting to random fluctuations  

ğŸ“Œ **For Analysts:**  
   - Complete t-test workflow example  
   - Proper assumption checking techniques  

ğŸ“Œ **For Store Managers:**  
   - Understands true performance differences  
   - Guides resource allocation decisions  

### ğŸ“ˆ Statistical Tests Performed  
1. **Normality Testing:**  
   - Shapiro-Wilk Test (Both regions p > 0.05 â†’ Normally distributed)  

2. **Equal Variance Testing:**  
   - Levene's Test (p = 0.423 â†’ Equal variances assumed)  

3. **Two-Sample t-test Results:**  
   - t-statistic: -1.593  
   - p-value: 0.112  
   - 95% Confidence Interval: (-2193.5, 228.3)  

### ğŸ”‘ Key Findings  
â€¢ **p-value = 0.112** (> 0.05 threshold)  
â€¢ **Conclusion:** Fail to reject null hypothesis  

### Business Interpretation  

ğŸ“¢**Despite the $1,000 higher average revenue in South region, this difference is **not statistically significant**. Regional strategies should focus on factors beyond geography to improve revenue.**


---

## 4. Paired T-Test

ğŸ“„ **Project Link:** **[Paired T-Test.ipynb](Hypothesis%20Testing/Hypothesis%20Tests/2.%20Paired%20T-Test.ipynb)**

## **ğŸ“Š Paired T-Test Analysis: Measuring Ad Campaign Effectiveness** 

### ğŸŒŸ Overview  
This project uses a **Paired T-Test** to evaluate whether an advertising campaign significantly impacted product sales across 30 stores. By comparing pre-campaign and post-campaign sales data from the same stores, we:  
â€¢ Verified statistical assumptions  
â€¢ Calculated meaningful effect sizes  
â€¢ Provided actionable business recommendations  

### â“ Problem Statement  
"Did our recent marketing campaign actually increase store sales, or are the observed improvements just random fluctuations?"  

### ğŸ¯ Objective  
â€¢ Quantify the campaign's impact with 95% confidence  
â€¢ Demonstrate proper paired test methodology  
â€¢ Guide future marketing investment decisions  

### ğŸ“Š Dataset Description  
**Synthetic Sales Data (30 Stores):**  
â€¢ **Before Campaign:**  
  - Mean Sales: 200 units  
  - Standard Deviation: 20 units  
â€¢ **After Campaign:**  
  - Mean Sales: ~210 units (+5% lift)  
  - Added Noise: N(10,15)  

**Key Variables:**  
â€¢ `Store_ID`: Unique identifier  
â€¢ `Before_Campaign`: Daily avg sales (pre-intervention)  
â€¢ `After_Campaign`: Daily avg sales (post-intervention)  

### ğŸš€ Project Highlights  
âœ… **Assumption Validation:**  
   - Shapiro-Wilk test (p=0.913 â†’ normally distributed differences)  
   - Visual checks via Q-Q plots and histograms  

âœ… **Robust Analysis:**  
   - Proper handling of paired observations  
   - Correct interpretation of directional change  

âœ… **Business-Ready Insights:**  
   - Clear effect size calculation  
   - Statistical significance with practical interpretation  

### ğŸ” Why This Matters  
â€¢ Real-world example of **within-subjects** analysis  
â€¢ Shows why paired tests outperform independent tests for before/after studies  
â€¢ Provides template for marketing ROI measurement  

### ğŸ’¡ Value Proposition  
ğŸ“Œ **For Marketing Teams:**  
   - Proof of campaign effectiveness  
   - Justification for future budgets  

ğŸ“Œ **For Store Managers:**  
   - Quantified performance impact  
   - Benchmark for future initiatives  

ğŸ“Œ **For Data Teams:**  
   - Complete paired test workflow  
   - Assumption checking best practices  

### ğŸ“ˆ Statistical Approach  
1. **Normality Testing:**  
   - Shapiro-Wilk on paired differences (p > 0.05 â†’ normal)  

2. **Paired T-Test Results:**  
   - t-statistic: 3.209  
   - p-value: 0.0032  
   - Mean Difference: +10 units  
   - 95% CI: [3.8, 16.2]  

### ğŸ”‘ Key Findings  
â€¢ **Highly Significant Result** (p = 0.0032)  
â€¢ **Average Lift:** +10 units per store (5% increase)  
â€¢ **Confidence:** 95% certain true effect is between +3.8 to +16.2 units  

### Executive Conclusion  
ğŸ“¢ **The advertising campaign **successfully increased sales** by an average of 10 units per store (p < 0.01). This statistically significant result suggests the campaign should be considered for scaling to other markets.**


---

## 5. ANOVA

ğŸ“„ **Project Link:** **[ANOVA Test](Hypothesis%20Testing/Hypothesis%20Tests/3.%20ANOVA%20Test.ipynb)**

## ğŸ“Š **ANOVA Analysis: Comparing Marketing Campaign Effectiveness**  

### ğŸŒŸ Overview  
This project employs **One-Way ANOVA** to evaluate whether three marketing campaigns (Email, Social Media, TV) generate significantly different revenue outcomes. Through rigorous assumption checks and statistical testing, we provide data-driven insights for marketing strategy optimization.

### â“ Problem Statement  
"Do our different marketing campaigns (Email vs. Social Media vs. TV) produce meaningfully different revenue results, or are observed variations just random?"

### ğŸ¯ Objective  
â€¢ Determine if campaign type affects revenue generation  
â€¢ Validate ANOVA assumptions for reliable results  
â€¢ Guide future marketing budget allocation  

### ğŸ“Š Dataset Description  
**Synthetic Revenue Data (90 Customers):**  
â€¢ **Email Campaign (30):**  
  - Mean: $250  
  - SD: $30  
â€¢ **Social Media (30):**  
  - Mean: $270  
  - SD: $25  
â€¢ **TV Campaign (30):**  
  - Mean: $260  
  - SD: $35  

**Variables:**  
â€¢ `campaign_type`: Categorical (3 levels)  
â€¢ `revenue`: Continuous numerical values  

### ğŸš€ Project Highlights  
âœ… **Comprehensive Assumption Checks:**  
   - Normality per group (Shapiro-Wilk all p > 0.05)  
   - Homogeneity of variance (Levene's p = 0.084)  
   - Independence by design  

âœ… **Robust Statistical Testing:**  
   - Proper ANOVA implementation  
   - Effect size calculation (Î·Â² optional)  

âœ… **Actionable Visualization:**  
   - Boxplots showing revenue distributions  
   - Q-Q plots for normality verification  

### ğŸ” Why This Matters  
â€¢ Demonstrates proper **multi-group comparison** methodology  
â€¢ Highlights importance of **assumption validation** before ANOVA  
â€¢ Provides framework for **marketing ROI analysis**  

### ğŸ’¡ Business Value  
ğŸ“Œ **For CMOs:**  
   - Data-backed campaign evaluation  
   - Objective budget allocation guidance  

ğŸ“Œ **For Marketing Teams:**  
   - Identifies equally effective channels  
   - Prevents over-investment in underperforming campaigns  

ğŸ“Œ **For Data Teams:**  
   - Complete ANOVA workflow example  
   - Assumption checking best practices  

### ğŸ“ˆ Statistical Approach  
1. **Assumption Verification:**  
   - Shapiro-Wilk normality tests (All groups p > 0.05)  
   - Levene's test for equal variances (p = 0.084)  

2. **One-Way ANOVA Results:**  
   - F-statistic: 1.699  
   - p-value: 0.189  
   - Group Means:  
     â€¢ Email: $250  
     â€¢ Social: $270  
     â€¢ TV: $260  

### ğŸ”‘ Key Findings  
â€¢ **High p-value (0.189 > 0.05)** â†’ Fail to reject null  
â€¢ **No significant revenue differences** between campaigns  
â€¢ Observed $20 variations likely due to random chance  

###  Strategic Conclusion  
ğŸ“¢ **All three campaigns perform **statistically similarly** in revenue generation (p = 0.19). Marketing budgets could be allocated based on other factors like customer reach or cost efficiency rather than expected revenue differences.**

---

## 6. Correlation Coefficient

ğŸ“„ **Project Link:** **[Correlation_Coefficient.ipynb](Hypothesis%20Testing/Hypothesis%20Tests/4_Correlation_Coefficient.ipynb)**

## ğŸ“ˆ **Correlation Analysis: Advertising Spend vs Revenue** 

### ğŸŒŸ Overview  
This project investigates the relationship between advertising expenditure and revenue generation using Pearson's correlation coefficient. Through synthetic data modeling and rigorous statistical testing, we determine whether increased ad spending reliably predicts higher revenue.

### â“ Problem Statement  
"Does increasing our advertising budget lead to proportionally higher revenue, or are other factors more influential?"  

### ğŸ¯ Objective  
â€¢ Quantify the ad spend-revenue relationship  
â€¢ Validate statistical assumptions for correlation analysis  
â€¢ Provide data-driven marketing investment guidance  

### ğŸ“Š Dataset Description  
**Synthetic Marketing Data (100 Weeks):**  
â€¢ **Advertising Spend:**  
  - Mean: $10,000  
  - SD: $2,000  
  - Normally distributed  
â€¢ **Revenue:**  
  - Weak linear relationship (5% of ad spend)  
  - Added noise: N(500,500)  

**Variables:**  
â€¢ `ad_spend`: Continuous numerical (USD)  
â€¢ `revenue`: Continuous numerical (USD)  

### ğŸš€ Project Highlights  
âœ… **Comprehensive Assumption Checks:**  
   - Normality confirmed (Shapiro-Wilk p > 0.05)  
   - Variance homogeneity verified  

âœ… **Robust Statistical Testing:**  
   - Pearson's r calculation  
   - Two-tailed significance testing  

âœ… **Practical Interpretation:**  
   - Business-focused conclusions  
   - Clear effect size reporting  

### ğŸ” Why This Matters  
â€¢ Demonstrates proper **bivariate correlation** analysis  
â€¢ Highlights importance of **testing assumptions**  
â€¢ Provides framework for **marketing ROI evaluation**  

### ğŸ’¡ Business Value  
ğŸ“Œ **For CFOs:**  
   - Evidence for budget allocation decisions  
   - Identifies weak spending-revenue linkages  

ğŸ“Œ **For Marketing Teams:**  
   - Guides campaign optimization strategies  
   - Supports case for testing alternative channels  

ğŸ“Œ **For Data Teams:**  
   - Complete correlation analysis template  
   - Assumption validation best practices  

### ğŸ“ˆ Statistical Approach  
1. **Assumption Verification:**  
   - Normality (Shapiro-Wilk p = 0.6552 & 0.2083)  
   - Homogeneity (Levene's p < 0.001)  

2. **Pearson Correlation Results:**  
   - r = 0.0545 (Very weak positive)  
   - p-value = 0.5904  
   - 95% CI: [-0.14, 0.24]  

### ğŸ”‘ Key Findings  
â€¢ **Statistically Insignificant** relationship (p > 0.05)  
â€¢ **Negligible Effect Size** (r < 0.1)  
â€¢ Only **5.4% of revenue variance** explained by ad spend  

### ğŸ“¢ Strategic Conclusion  

**No significant linear relationship exists between advertising spend and revenue (p = 0.59). Businesses should:**  
1ï¸âƒ£ **Investigate other revenue drivers**  
2ï¸âƒ£ **Optimize existing campaigns before increasing budgets**  
3ï¸âƒ£ **Test alternative marketing channels**


---

### 7. Mann-Whitney U Test

ğŸ“„ **Project Link:** **[Mann-Whitney U Test.ipynb](Hypothesis%20Testing/Hypothesis%20Tests/5.%20Mann-Whitney%20U%20Test.ipynb)**

## ğŸ“Š **Non-Parametric Comparison: North vs South Sales Performance**  

### ğŸŒŸ Overview  
This project employs the **Mann-Whitney U Test** to compare sales revenue distributions between two retail regions without normality assumptions. Using synthetic gamma-distributed data, we demonstrate proper non-parametric analysis workflow from assumption checks to business interpretation.

### â“ Problem Statement  
"Is there a statistically significant difference in sales performance between our North and South regions, given non-normal revenue distributions?"  

### ğŸ¯ Objective  
â€¢ Compare regional sales distributions robustly  
â€¢ Demonstrate non-parametric testing methodology  
â€¢ Guide regional strategy decisions with data  

### ğŸ“Š Dataset Description  
**Synthetic Revenue Data (200 Records):**  
â€¢ **North Region (100 stores):**  
  - Gamma distribution (shape=2, scale=500)  
  - Positively skewed  
â€¢ **South Region (100 stores):**  
  - Gamma distribution (shape=2, scale=520)  
  - Slightly higher potential values  

**Variables:**  
â€¢ `region`: Categorical (North/South)  
â€¢ `revenue`: Continuous numerical values  

### ğŸš€ Project Highlights  
âœ… **Proper Assumption Handling:**  
   - Confirmed non-normality (Shapiro-Wilk p < 0.001)  
   - Verified equal variances (Levene's p = 0.198)  

âœ… **Robust Statistical Testing:**  
   - Correct application of Mann-Whitney U test  
   - Clear interpretation of rank-based comparison  

âœ… **Actionable Business Insights:**  
   - Translates statistical results to strategic guidance  
   - Provides framework for regional performance analysis  

### ğŸ” Why This Matters  
â€¢ Real-world demonstration of **non-parametric testing**  
â€¢ Shows alternative when t-test assumptions fail  
â€¢ Provides template for **skewed data comparisons**  

### ğŸ’¡ Business Value  
ğŸ“Œ **For Regional Managers:**  
   - Evidence-based performance evaluation  
   - Avoids misleading normal-distribution assumptions  

ğŸ“Œ **For Data Teams:**  
   - Complete non-parametric analysis example  
   - Proper assumption validation techniques  

ğŸ“Œ **For Executives:**  
   - Confidence in regional comparisons  
   - Guides resource allocation decisions  

### ğŸ“ˆ Statistical Approach  
1. **Assumption Verification:**  
   - Shapiro-Wilk: Confirmed non-normality (p < 0.001)  
   - Levene's Test: Equal variances (p = 0.198)  

2. **Mann-Whitney U Test Results:**  
   - U-statistic: 4549.0  
   - p-value: 0.2710  
   - Effect Size: Rank-biserial correlation optional  

### ğŸ”‘ Key Findings  
â€¢ **High p-value (0.271 > 0.05)** â†’ Fail to reject null  
â€¢ **No significant difference** in revenue distributions  
â€¢ Median revenues:  
  - North: $963.42  
  - South: $996.87  

### ğŸ“¢ Strategic Conclusion  

**No statistically significant difference exists between North and South sales distributions (p = 0.27). Regional strategies should:** 

1ï¸âƒ£ **Focus on non-geographic performance drivers**
2ï¸âƒ£ **Investigate higher-level business factors**  
3ï¸âƒ£ **Maintain consistent policies across regions**


---

## 8. Wilcoxon Signed-Rank Test

ğŸ“„ **Project Link:** **[Wilcoxon Signed-Rank Test.ipynb](Hypothesis%20Testing/Hypothesis%20Tests/6.%20Wilcoxon%20Signed-Rank%20Test.ipynb)**

## ğŸ“ˆ Wilcoxon Signed-Rank Test: Promotional Campaign Impact Analysis  

### ğŸŒŸ Overview  
This project applies the **Wilcoxon Signed-Rank Test** to evaluate whether a promotional campaign significantly increased daily sales. Using synthetic paired data that violates normality assumptions, we demonstrate proper non-parametric analysis for before-after studies.

### â“ Problem Statement  
"Did our recent promotional campaign actually increase sales, given that our sales data isn't normally distributed?"  

### ğŸ¯ Objective  
â€¢ Quantify campaign impact without normality assumptions  
â€¢ Demonstrate robust paired data analysis  
â€¢ Provide actionable marketing insights  

### ğŸ“Š Dataset Description  
**Synthetic Sales Data (60 Days):**  
â€¢ **Before Campaign:**  
  - Gamma distribution (shape=2, scale=100)  
  - Median: $191.42  
â€¢ **After Campaign:**  
  - Before sales + N(10,15) boost  
  - Median: $200.87 (+4.9%)  

**Variables:**  
â€¢ `before_sales`: Daily sales pre-campaign  
â€¢ `after_sales`: Daily sales post-campaign  

### ğŸš€ Project Highlights  
âœ… **Assumption Validation:**  
   - Confirmed non-normality of differences (Shapiro-Wilk p=0.295)  
   - Proper handling of paired observations  

âœ… **Robust Statistical Testing:**  
   - Correct application of Wilcoxon signed-rank test  
   - Effect size calculation (optional)  

âœ… **Business-Ready Insights:**  
   - Clear interpretation of median differences  
   - Statistical significance with practical implications  

### ğŸ” Why This Matters  
â€¢ Real-world demonstration of **non-parametric paired testing**  
â€¢ Shows alternative when paired t-test assumptions fail  
â€¢ Provides template for **marketing intervention analysis**  

### ğŸ’¡ Business Value  
ğŸ“Œ **For Marketing Teams:**  
   - Proof of campaign effectiveness  
   - Justification for future promotions  

ğŸ“Œ **For Store Managers:**  
   - Quantified sales impact  
   - Benchmark for future initiatives  

ğŸ“Œ **For Data Teams:**  
   - Complete non-parametric workflow  
   - Best practices for skewed paired data  

### ğŸ“ˆ Statistical Approach  
1. **Assumption Verification:**  
   - Shapiro-Wilk on differences (p=0.295 â†’ non-normal)  

2. **Wilcoxon Signed-Rank Results:**  
   - Test Statistic: 309.0  
   - p-value: <0.001  
   - Median Increase: +$9.45 (4.9%)  

### ğŸ”‘ Key Findings  
â€¢ **Highly Significant Result** (p < 0.0001)  
â€¢ **Median Sales Increase**: +4.9%  
â€¢ **Practical Impact**: Campaign successfully boosted sales  

### ğŸ“¢ Executive Conclusion  
**The promotional campaign **significantly increased daily sales** by a median of 4.9% (p < 0.001). This strong evidence supports:**  
1ï¸âƒ£ **Repeating similar campaigns**  
2ï¸âƒ£ **Allocating budget for promotions**  
3ï¸âƒ£ **Further testing to optimize impact**

---

## 9. Kruskal-Wallis Test

ğŸ“„ **Project Link:** **[Kruskal-Wallis Test](Hypothesis%20Testing/Hypothesis%20Tests/7.%20Kruskal-Wallis%20Test.ipynb)**

## ğŸ“Š **Kruskal-Wallis Test: Comparing Marketing Channel Performance**  

### ğŸŒŸ Overview  
This project applies the **Kruskal-Wallis H-test** to evaluate whether sales performance differs across three marketing channels (Email, Social Media, TV) when data violates normality assumptions. The analysis provides robust insights for marketing budget allocation decisions.

### â“ Problem Statement  
"Do our marketing channels (Email vs Social Media vs TV) generate significantly different sales results, given non-normal revenue distributions?"  

### ğŸ¯ Objective  
â€¢ Compare channel performance without normality assumptions  
â€¢ Demonstrate proper non-parametric multi-group analysis  
â€¢ Guide marketing budget allocation decisions  

### ğŸ“Š Dataset Description  
**Synthetic Sales Data (150 Records):**  
â€¢ **Email Campaign (50):**  
  - Gamma distribution (shape=2, scale=120)  
  - Median: $229.18  
â€¢ **Social Media (50):**  
  - Gamma distribution (shape=2, scale=110)  
  - Median: $210.65  
â€¢ **TV Campaign (50):**  
  - Gamma distribution (shape=2, scale=140)  
  - Median: $268.94  

**Variables:**  
â€¢ `channel`: Categorical (Email/Social/TV)  
â€¢ `sales`: Continuous numerical values  

### ğŸš€ Project Highlights  
âœ… **Comprehensive Assumption Checks:**  
   - Confirmed non-normality for all channels (Shapiro-Wilk p < 0.05)  
   - Verified ordinal data structure  

âœ… **Robust Statistical Testing:**  
   - Correct application of Kruskal-Wallis test  
   - Post-hoc analysis available if significant  

âœ… **Actionable Business Insights:**  
   - Clear interpretation of median sales  
   - Statistical significance with practical implications  

### ğŸ” Why This Matters  
â€¢ Real-world demonstration of **non-parametric ANOVA alternative**  
â€¢ Shows proper handling of **skewed multi-group data**  
â€¢ Provides template for **marketing channel evaluation**  

### ğŸ’¡ Business Value  
ğŸ“Œ **For CMOs:**  
   - Data-driven channel performance assessment  
   - Objective budget allocation guidance  

ğŸ“Œ **For Marketing Teams:**  
   - Identifies equally effective channels  
   - Prevents over-investment in underperforming channels  

ğŸ“Œ **For Data Teams:**  
   - Complete non-parametric workflow  
   - Best practices for ordinal data analysis  

### ğŸ“ˆ Statistical Approach  
1. **Assumption Verification:**  
   - Shapiro-Wilk tests (All p < 0.05 â†’ non-normal)  

2. **Kruskal-Wallis Test Results:**  
   - H-statistic: 2.6024  
   - p-value: 0.2722  
   - Group Medians:  
     â€¢ Email: $229.18  
     â€¢ Social: $210.65  
     â€¢ TV: $268.94  

### ğŸ”‘ Key Findings  
â€¢ **High p-value (0.272 > 0.05)** â†’ Fail to reject null  
â€¢ **No significant difference** in sales distributions  
â€¢ Observed median differences likely due to random variation  

### ğŸ“¢ Strategic Conclusion  
**All three marketing channels perform **statistically similarly** in sales generation (p = 0.27). Recommendations:**  
1ï¸âƒ£ **Allocate budgets based on cost-efficiency rather than sales differences**  
2ï¸âƒ£ **Investigate other performance metrics (ROI, customer acquisition cost)**  
3ï¸âƒ£ **Consider testing additional channels or campaign variations**

---

## 10. Chi-Square Goodness of Fit Test

ğŸ“„ **Project Link:** **[Chi-Square Goodness.ipynb
](Hypothesis%20Testing/Hypothesis%20Tests/8.%20Chi-Square%20Goodness.ipynb)**

ğŸ“Š Chi-Square Goodness of Fit: Marketing Campaign Analysis  

### ğŸŒŸ Overview  
This project applies the **Chi-Square Goodness of Fit Test** to evaluate whether observed customer responses across three marketing campaigns match expected historical patterns. The analysis helps validate marketing channel performance assumptions.

### â“ Problem Statement  
"Does our current customer response distribution across Email, Social Media, and TV campaigns differ significantly from what we historically expect?"  

### ğŸ¯ Objective  
â€¢ Test alignment between observed and expected customer distributions  
â€¢ Demonstrate proper Chi-Square Goodness of Fit implementation  
â€¢ Provide data-driven insights for campaign evaluation  

### ğŸ“Š Dataset Description  
**Synthetic Customer Response Data:**  
â€¢ **Observed Counts:**  
  - Email: 120 customers  
  - Social Media: 95 customers  
  - TV: 85 customers  
â€¢ **Expected Proportions:**  
  - Email: 40%  
  - Social Media: 35%  
  - TV: 25%  

**Total Customers:** 300  

### ğŸš€ Project Highlights  
âœ… **Proper Test Selection:**  
   - Designed for categorical frequency data  
   - Non-parametric (no normality assumptions)  

âœ… **Rigorous Assumption Checks:**  
   - Verified expected frequencies â‰¥5  
   - Confirmed independence of observations  

âœ… **Clear Interpretation:**  
   - Business-focused conclusions  
   - Practical significance assessment  

### ğŸ” Why This Matters  
â€¢ Demonstrates **distribution validation** technique  
â€¢ Provides framework for **marketing performance benchmarking**  
â€¢ Shows proper **categorical data analysis** approach  

### ğŸ’¡ Business Value  
ğŸ“Œ **For Marketing Teams:**  
   - Validates campaign performance assumptions  
   - Guides resource allocation decisions  

ğŸ“Œ **For Data Analysts:**  
   - Complete Chi-Square workflow example  
   - Expected frequency calculation method  

ğŸ“Œ **For Executives:**  
   - Confidence in campaign distribution patterns  
   - Evidence for strategic planning  

### ğŸ“ˆ Statistical Approach  
1. **Assumption Verification:**  
   - All expected counts â‰¥5 (Met)  
   - Independent observations (By design)  

2. **Chi-Square Test Results:**  
   - Ï‡Â² Statistic: 2.2857  
   - p-value: 0.3189  
   - Degrees of Freedom: 2  

### ğŸ”‘ Key Findings  
â€¢ **High p-value (0.319 > 0.05)** â†’ Fail to reject null  
â€¢ **No significant difference** between observed/expected  
â€¢ Current campaign responses match historical patterns  

### ğŸ“¢ Strategic Conclusion  
**Customer responses align with historical distributions across all channels (p = 0.32). Recommended actions:**  
1ï¸âƒ£ **Maintain current campaign mix**  
2ï¸âƒ£ **Monitor for future deviations**  
3ï¸âƒ£ **Investigate absolute performance metrics**

---

### ğŸ› ï¸ Technical Implementation  
   - Python (Pandas, NumPy)  
   - SciPy (kruskal, shapiro)  
   - Matplotlib/Seaborn (Visualizations)


---

## How to Run These Projects

1. Open the project in [Google Colab](https://colab.research.google.com/)
2. Upload your Excel dataset when prompted.
3. Run all cells in order:
   - Data loading
   - EDA
   - Contingency table creation
   - Visualization
   - Statistical testing
4. Review the final statistical output and visualizations.

---

## Conclusion

This collection of projects provides a foundational understanding of diverse hypothesis testing methods applicable in business contexts. By rigorously checking assumptions and applying appropriate parametric or non-parametric tests, these projects showcase how statistical testing can validate business hypotheses, support decision-making, and uncover actionable insights.

---

