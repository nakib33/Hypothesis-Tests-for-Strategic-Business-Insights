# üöÄ Hypothesis Tests for Strategic Business Insights


---

## üìå Description

---

This repository contains 10 comprehensive projects focused on applying various statistical hypothesis tests to generate strategic business insights. Each project uses synthetic or real datasets to demonstrate how hypothesis testing can aid in making informed business decisions by validating assumptions, comparing groups, and identifying relationships between variables. The projects span parametric and non-parametric tests to handle different data characteristics and assumptions, providing a robust toolkit for data-driven analysis.

## üìÅ Table of Contents
- [Description](#description)
- [Projects Overview](#projects-overview)
- [Detailed Project Information](#detailed-project-information)
  - [1. A/B Testing](#1-ab-testing)
  - [2. Chi-Square Test](#2-chi-square-test)
  - [3. Two-Sample T-Test](#3-two-sample-t-test)
  - [4. Paired T-Test](#4-paired-t-test)
  - [5. ANOVA](#5-anova)
  - [6. Correlation Coefficient](#6-correlation-coefficient)
  - [7. Mann-Whitney U Test](#7-mann-whitney-u-test)
  - [8. Wilcoxon Signed-Rank Test](#8-wilcoxon-signed-rank-test)
  - [9. Kruskal-Wallis Test](#9-kruskal-wallis-test)
  - [10. Chi-Square Goodness of Fit Test](#10-chi-square-goodness-of-fit-test)
- [Technical Implementation](#technical-implementation)
- [How to Run These Projects](#how-to-run-these-projects)
- [Conclusion](#conclusion)



---

## Projects Overview

| Project No. | Project Title                                               |
|-------------|-------------------------------------------------------------|
| 1           | A/B Testing                                                 |
| 2           | Chi-Square Test                                            |
| 3           | Two-Sample T-Test                                          |
| 4           | Paired T-Test                                              |
| 5           | ANOVA                                                      |
| 6           | Correlation Coefficient                                    |
| 7           | Mann-Whitney U Test (Non-parametric alternative to Two-Sample t-test) |
| 8           | Wilcoxon Signed-Rank Test (Non-parametric paired test)    |
| 9           | Kruskal-Wallis Test (Non-parametric ANOVA)                |
| 10          | Chi-Square Goodness of Fit Test                            |

---

## Detailed Project Information

## 1. A/B Testing
üìÑ **Project Link:** **[A_B_Testing_Project.ipynb](Hypothesis%20Testing/AB%20Testing/A_B_Testing_Project.ipynb)**

## A/B Testing Project: Evaluating Button Color Impact on User Conversions  

### üìå Overview  
This project conducts a rigorous A/B testing analysis to evaluate the impact of two different button color variants (Blue vs. Green) on user conversion rates for an e-commerce platform. Using a real-world dataset from Kaggle, the study follows a structured approach, including:  
- **Exploratory Data Analysis (EDA)**  
- **Assumption Verification** (Normality & Homogeneity of Variances)  
- **Statistical Testing** (Independent Samples t-test & Mann-Whitney U Test)  
- **Effect Size & Power Analysis**  
- **Result Interpretation & Business Recommendations**  

üîó **Dataset Source**: [A/B Testing for Button Color Variants Dataset (Kaggle)](https://www.kaggle.com/datasets/example/ab-testing-button-color)  


### ‚ùì Problem Statement  
E-commerce platforms continuously experiment with UI elements (like button colors) to maximize user engagement and conversions. However, without proper statistical validation, design changes may not yield meaningful improvements.  

**Key Question**:  
*Does changing the button color (from Blue to Green) lead to a statistically significant increase in user conversion rates?*  


### üéØ Objective  
- **Determine** if there is a statistically significant difference in conversion rates between the two button colors.  
- **Assess** the practical significance of the change using effect size and power analysis.  
- **Provide** data-driven recommendations for UI optimization.  


### üìä Dataset Description  
The dataset contains synthetic user interaction data simulating an A/B test with two button variants:  
- **Control Group (A)**: Blue button  
- **Treatment Group (B)**: Green button  

**Key Features**:  
- `user_id`: Unique identifier for each user  
- `group`: Indicates whether the user was in Group A (Control) or Group B (Treatment)  
- `clicked`: Binary indicator (1 = clicked, 0 = did not click)  
- `converted`: Binary indicator (1 = converted, 0 = did not convert)  

**Size**: <1MB (Ideal for quick analysis)  


### üöÄ Project Highlights  
‚úÖ **Comprehensive EDA**: Visualized conversion rates, checked distributions (histograms, Q-Q plots), and compared engagement metrics.  
‚úÖ **Assumption Checks**: Verified normality (Shapiro-Wilk) and homogeneity of variances (Levene‚Äôs test).  
‚úÖ **Statistical Testing**:  
   - **Independent Samples t-test** (for normally distributed data)  
   - **Mann-Whitney U Test** (non-parametric alternative)  
‚úÖ **Effect Size & Power Analysis**: Calculated Cohen‚Äôs d (effect size) and statistical power to assess practical significance.  
‚úÖ **Actionable Insights**: Provided clear recommendations based on statistical evidence.  


### üîç Why This Project?  
- Demonstrates **real-world A/B testing methodology** from hypothesis formulation to result interpretation.  
- Highlights **importance of statistical validation** before implementing UI changes.  
- Provides a **template for data-driven decision-making** in marketing & UX optimization.  


### üí° Benefits / Use Cases  
üìå **For Businesses**:  
- Helps optimize UI elements to **increase conversions & revenue**.  
- Reduces guesswork by relying on **statistically validated insights**.  

üìå **For Data Scientists**:  
- Serves as a **reference for A/B testing best practices**.  
- Demonstrates **assumption checks, test selection, and interpretation**.  

üìå **For Product Teams**:  
- Guides **data-backed design decisions** for better user engagement.  


### üìà Statistical Tests Used  
1. **Normality Check**: Shapiro-Wilk Test  
2. **Equal Variance Check**: Levene‚Äôs Test  
3. **Hypothesis Testing**:  
   - **Independent Samples t-test** (Parametric)  
   - **Mann-Whitney U Test** (Non-parametric)  
4. **Effect Size**: Cohen‚Äôs d  
5. **Power Analysis**: Statistical Power (1 - Œ≤)  

### üîë Key Findings & Result Summary  
### Hypothesis Testing Results  
- **p-value (t-test)**: **0.927** (Fail to reject H‚ÇÄ)  
- **p-value (Mann-Whitney U)**: **0.928** (Fail to reject H‚ÇÄ)  
- **Conclusion**: **No statistically significant difference** in conversion rates between Blue & Green buttons.  

### Effect Size & Power  
- **Cohen‚Äôs d**: **0.500** (Medium effect size)  
- **Statistical Power**: **0.85** (Adequate to detect an effect)  

### Final Recommendation  
üì¢ **"No significant improvement in conversions was observed when changing the button color from Blue to Green. Thus, retaining the original Blue button is recommended unless further tests with different variations suggest otherwise."**


---

## 2. Chi-Square Test

## **Chi-Square Test Project: Gender vs Purchase Likelihood Analysis**  

üìÑ **Project Link:** **[Chi_Square_Testing.ipynb](Hypothesis%20Testing/Chi-Square%20Test/Chi_Square_Testing.ipynb)**

### üìå Overview  
This project employs the **Chi-Square Test of Independence** to investigate whether a statistically significant association exists between customer gender and product purchase likelihood in an e-commerce context. Using a carefully crafted synthetic dataset that mimics real-world shopping behavior, the analysis includes:  
- Data visualization of key variables (gender, purchase status, age)  
- Contingency table construction  
- Chi-Square test assumption verification  
- Statistical testing and interpretation  
- Actionable business insights  


###  ‚ùì Problem Statement  
E-commerce businesses often wonder if demographic factors like gender influence purchasing decisions. This analysis answers:  
**"Does customer gender significantly affect the likelihood of purchasing our product?"**  


###  üéØ Objective  
- Determine if gender and purchase likelihood are statistically associated  
- Validate assumptions for Chi-Square testing  
- Provide data-driven recommendations for marketing strategies  


###  üìä Dataset Description  
**Synthetic Dataset** (500 records) simulating real customer behavior:  
- `Gender`: Binary (Male/Female) with near-equal distribution  
- `Purchase`: Binary (Yes/No) with gender-biased probabilities  
- `Age`: Normally distributed (Œº=35, œÉ=10), clipped to 18-70 years  

**Key Characteristics**:  
‚úÖ Balanced gender distribution (Male: 233, Female: 267)  
‚úÖ Realistic purchase probability variation (Male: 45%, Female: 55%)  
‚úÖ Numeric age variable for additional demographic analysis  


###  üöÄ Project Highlights  
‚úÖ **Comprehensive EDA**: Countplots, histograms, and contingency tables  
‚úÖ **Assumption Verification**:  
   - Independence of observations  
   - Expected frequencies ‚â•5 (all cells satisfied)  
‚úÖ **Statistical Testing**:  
   - Chi-Square Test of Independence  
   - Effect size calculation (Cramer's V optional)  
‚úÖ **Clear Interpretation**: Business-focused conclusions  


###  üîç Why This Project?  
- Demonstrates **real-world application** of Chi-Square testing  
- Highlights **importance of assumption checks** in statistical analysis  
- Provides **template for customer segmentation studies**  


###  üí° Benefits / Use Cases  
üìå **For Marketing Teams**:  
- Avoids gender-based assumptions in campaigns  
- Guides resource allocation for promotions  

üìå **For Data Scientists**:  
- Complete workflow from synthetic data generation to interpretation  
- Best practices for categorical data analysis  

üìå **For Product Managers**:  
- Evidence-based decisions on product positioning  


###  üìà Statistical Tests Used  
1. **Chi-Square Test of Independence**  
   - Test Statistic: œá¬≤ = 1.9395  
   - Degrees of Freedom: 1  
   - p-value: 0.1641  
2. **Expected Frequency Verification**  
3. **Effect Size Measurement** (Optional: Cramer's V/Phi Coefficient)  


### **üîë Key Findings & Result Summary**  
**Hypothesis Testing Results:**  
- **p-value**: 0.1641 (> 0.05 significance level)  
- **Conclusion**: Fail to reject null hypothesis  

**Business Interpretation:** 
üì¢ **"No statistically significant association found between gender and purchase likelihood. Marketing strategies should not prioritize gender-based targeting for this product."**  

### Supplementary Insights  
- Observed purchase rates:  
  - Female: 55%  
  - Male: 45%  
- While numeric differences exist, they're **not statistically significant**
  
---

## 3. Two-Sample T-Test

## üìä **Two-Sample T-Test: Revenue Comparison Between North and South Regions**  

üìÑ **Project Link:** **[Two-Sample T-Test.ipynb](Hypothesis%20Testing/Hypothesis%20Tests/1.%20Two-Sample%20T-Test.ipynb)**

### üåê Overview  
This project performs a Two-Sample Independent t-test to analyze whether there's a statistically significant difference in average revenue between North and South regions of a retail business. Using a carefully generated synthetic dataset, we:  
‚Ä¢ Visualize revenue distributions  
‚Ä¢ Validate statistical assumptions  
‚Ä¢ Conduct hypothesis testing  
‚Ä¢ Derive business insights  

### ‚ùì Problem Statement  
A retail chain observes slight revenue differences between its North and South locations. Management needs to know:  
*"Is this observed difference statistically significant, or just random variation?"*  

### üéØ Objective  
‚Ä¢ Determine if regional revenue differences are statistically meaningful  
‚Ä¢ Demonstrate proper t-test application with assumption checks  
‚Ä¢ Provide data-backed recommendations for regional strategy  

### üìä Dataset Description  
**Synthetic Revenue Data (500 records):**  
‚Ä¢ **North Region (250 samples):**  
  - Mean: $50,000  
  - Standard Deviation: $8,000  
‚Ä¢ **South Region (250 samples):**  
  - Mean: $51,000  
  - Standard Deviation: $8,500  

**Variables:**  
‚Ä¢ `region`: Categorical (North/South)  
‚Ä¢ `revenue`: Continuous numerical values  

### üöÄ Project Highlights  
‚úÖ **Data Visualization:**  
   - Histograms with KDE plots  
   - Q-Q plots for normality inspection  

‚úÖ **Rigorous Assumption Testing:**  
   - Shapiro-Wilk normality test (North p=0.126, South p=0.884)  
   - Levene's test for equal variances  

‚úÖ **Proper Statistical Testing:**  
   - Independent Two-Sample t-test implementation  
   - Correct interpretation of p-values  

‚úÖ **Business-Ready Conclusions:**  
   - Clear statistical findings translated to actionable insights  

### üîç Why This Project Matters  
‚Ä¢ Real-world demonstration of t-test methodology  
‚Ä¢ Emphasizes critical assumption validation step  
‚Ä¢ Provides template for regional performance analysis  

### üí° Business Value  
üìå **For Executives:**  
   - Evidence-based regional strategy decisions  
   - Avoids overreacting to random fluctuations  

üìå **For Analysts:**  
   - Complete t-test workflow example  
   - Proper assumption checking techniques  

üìå **For Store Managers:**  
   - Understands true performance differences  
   - Guides resource allocation decisions  

### üìà Statistical Tests Performed  
1. **Normality Testing:**  
   - Shapiro-Wilk Test (Both regions p > 0.05 ‚Üí Normally distributed)  

2. **Equal Variance Testing:**  
   - Levene's Test (p = 0.423 ‚Üí Equal variances assumed)  

3. **Two-Sample t-test Results:**  
   - t-statistic: -1.593  
   - p-value: 0.112  
   - 95% Confidence Interval: (-2193.5, 228.3)  

### üîë Key Findings  
‚Ä¢ **p-value = 0.112** (> 0.05 threshold)  
‚Ä¢ **Conclusion:** Fail to reject null hypothesis  

### Business Interpretation  

üì¢**Despite the $1,000 higher average revenue in South region, this difference is **not statistically significant**. Regional strategies should focus on factors beyond geography to improve revenue.**


---

## 4. Paired T-Test

üìÑ **Project Link:** **[Paired T-Test.ipynb](Hypothesis%20Testing/Hypothesis%20Tests/2.%20Paired%20T-Test.ipynb)**

## **üìä Paired T-Test Analysis: Measuring Ad Campaign Effectiveness** 

### üåü Overview  
This project uses a **Paired T-Test** to evaluate whether an advertising campaign significantly impacted product sales across 30 stores. By comparing pre-campaign and post-campaign sales data from the same stores, we:  
‚Ä¢ Verified statistical assumptions  
‚Ä¢ Calculated meaningful effect sizes  
‚Ä¢ Provided actionable business recommendations  

### ‚ùì Problem Statement  
"Did our recent marketing campaign actually increase store sales, or are the observed improvements just random fluctuations?"  

### üéØ Objective  
‚Ä¢ Quantify the campaign's impact with 95% confidence  
‚Ä¢ Demonstrate proper paired test methodology  
‚Ä¢ Guide future marketing investment decisions  

### üìä Dataset Description  
**Synthetic Sales Data (30 Stores):**  
‚Ä¢ **Before Campaign:**  
  - Mean Sales: 200 units  
  - Standard Deviation: 20 units  
‚Ä¢ **After Campaign:**  
  - Mean Sales: ~210 units (+5% lift)  
  - Added Noise: N(10,15)  

**Key Variables:**  
‚Ä¢ `Store_ID`: Unique identifier  
‚Ä¢ `Before_Campaign`: Daily avg sales (pre-intervention)  
‚Ä¢ `After_Campaign`: Daily avg sales (post-intervention)  

### üöÄ Project Highlights  
‚úÖ **Assumption Validation:**  
   - Shapiro-Wilk test (p=0.913 ‚Üí normally distributed differences)  
   - Visual checks via Q-Q plots and histograms  

‚úÖ **Robust Analysis:**  
   - Proper handling of paired observations  
   - Correct interpretation of directional change  

‚úÖ **Business-Ready Insights:**  
   - Clear effect size calculation  
   - Statistical significance with practical interpretation  

### üîç Why This Matters  
‚Ä¢ Real-world example of **within-subjects** analysis  
‚Ä¢ Shows why paired tests outperform independent tests for before/after studies  
‚Ä¢ Provides template for marketing ROI measurement  

### üí° Value Proposition  
üìå **For Marketing Teams:**  
   - Proof of campaign effectiveness  
   - Justification for future budgets  

üìå **For Store Managers:**  
   - Quantified performance impact  
   - Benchmark for future initiatives  

üìå **For Data Teams:**  
   - Complete paired test workflow  
   - Assumption checking best practices  

### üìà Statistical Approach  
1. **Normality Testing:**  
   - Shapiro-Wilk on paired differences (p > 0.05 ‚Üí normal)  

2. **Paired T-Test Results:**  
   - t-statistic: 3.209  
   - p-value: 0.0032  
   - Mean Difference: +10 units  
   - 95% CI: [3.8, 16.2]  

### üîë Key Findings  
‚Ä¢ **Highly Significant Result** (p = 0.0032)  
‚Ä¢ **Average Lift:** +10 units per store (5% increase)  
‚Ä¢ **Confidence:** 95% certain true effect is between +3.8 to +16.2 units  

### Executive Conclusion  
üì¢ **The advertising campaign **successfully increased sales** by an average of 10 units per store (p < 0.01). This statistically significant result suggests the campaign should be considered for scaling to other markets.**


---

## 5. ANOVA

üìÑ **Project Link:** **[ANOVA Test](Hypothesis%20Testing/Hypothesis%20Tests/3.%20ANOVA%20Test.ipynb)**

## üìä **ANOVA Analysis: Comparing Marketing Campaign Effectiveness**  

### üåü Overview  
This project employs **One-Way ANOVA** to evaluate whether three marketing campaigns (Email, Social Media, TV) generate significantly different revenue outcomes. Through rigorous assumption checks and statistical testing, we provide data-driven insights for marketing strategy optimization.

### ‚ùì Problem Statement  
"Do our different marketing campaigns (Email vs. Social Media vs. TV) produce meaningfully different revenue results, or are observed variations just random?"

### üéØ Objective  
‚Ä¢ Determine if campaign type affects revenue generation  
‚Ä¢ Validate ANOVA assumptions for reliable results  
‚Ä¢ Guide future marketing budget allocation  

### üìä Dataset Description  
**Synthetic Revenue Data (90 Customers):**  
‚Ä¢ **Email Campaign (30):**  
  - Mean: $250  
  - SD: $30  
‚Ä¢ **Social Media (30):**  
  - Mean: $270  
  - SD: $25  
‚Ä¢ **TV Campaign (30):**  
  - Mean: $260  
  - SD: $35  

**Variables:**  
‚Ä¢ `campaign_type`: Categorical (3 levels)  
‚Ä¢ `revenue`: Continuous numerical values  

### üöÄ Project Highlights  
‚úÖ **Comprehensive Assumption Checks:**  
   - Normality per group (Shapiro-Wilk all p > 0.05)  
   - Homogeneity of variance (Levene's p = 0.084)  
   - Independence by design  

‚úÖ **Robust Statistical Testing:**  
   - Proper ANOVA implementation  
   - Effect size calculation (Œ∑¬≤ optional)  

‚úÖ **Actionable Visualization:**  
   - Boxplots showing revenue distributions  
   - Q-Q plots for normality verification  

### üîç Why This Matters  
‚Ä¢ Demonstrates proper **multi-group comparison** methodology  
‚Ä¢ Highlights importance of **assumption validation** before ANOVA  
‚Ä¢ Provides framework for **marketing ROI analysis**  

### üí° Business Value  
üìå **For CMOs:**  
   - Data-backed campaign evaluation  
   - Objective budget allocation guidance  

üìå **For Marketing Teams:**  
   - Identifies equally effective channels  
   - Prevents over-investment in underperforming campaigns  

üìå **For Data Teams:**  
   - Complete ANOVA workflow example  
   - Assumption checking best practices  

### üìà Statistical Approach  
1. **Assumption Verification:**  
   - Shapiro-Wilk normality tests (All groups p > 0.05)  
   - Levene's test for equal variances (p = 0.084)  

2. **One-Way ANOVA Results:**  
   - F-statistic: 1.699  
   - p-value: 0.189  
   - Group Means:  
     ‚Ä¢ Email: $250  
     ‚Ä¢ Social: $270  
     ‚Ä¢ TV: $260  

### üîë Key Findings  
‚Ä¢ **High p-value (0.189 > 0.05)** ‚Üí Fail to reject null  
‚Ä¢ **No significant revenue differences** between campaigns  
‚Ä¢ Observed $20 variations likely due to random chance  

###  Strategic Conclusion  
üì¢ **All three campaigns perform **statistically similarly** in revenue generation (p = 0.19). Marketing budgets could be allocated based on other factors like customer reach or cost efficiency rather than expected revenue differences.**

---

## 6. Correlation Coefficient

üìÑ **Project Link:** **[Correlation_Coefficient.ipynb](Hypothesis%20Testing/Hypothesis%20Tests/4_Correlation_Coefficient.ipynb)**

## üìà **Correlation Analysis: Advertising Spend vs Revenue** 

### üåü Overview  
This project investigates the relationship between advertising expenditure and revenue generation using Pearson's correlation coefficient. Through synthetic data modeling and rigorous statistical testing, we determine whether increased ad spending reliably predicts higher revenue.

### ‚ùì Problem Statement  
"Does increasing our advertising budget lead to proportionally higher revenue, or are other factors more influential?"  

### üéØ Objective  
‚Ä¢ Quantify the ad spend-revenue relationship  
‚Ä¢ Validate statistical assumptions for correlation analysis  
‚Ä¢ Provide data-driven marketing investment guidance  

### üìä Dataset Description  
**Synthetic Marketing Data (100 Weeks):**  
‚Ä¢ **Advertising Spend:**  
  - Mean: $10,000  
  - SD: $2,000  
  - Normally distributed  
‚Ä¢ **Revenue:**  
  - Weak linear relationship (5% of ad spend)  
  - Added noise: N(500,500)  

**Variables:**  
‚Ä¢ `ad_spend`: Continuous numerical (USD)  
‚Ä¢ `revenue`: Continuous numerical (USD)  

### üöÄ Project Highlights  
‚úÖ **Comprehensive Assumption Checks:**  
   - Normality confirmed (Shapiro-Wilk p > 0.05)  
   - Variance homogeneity verified  

‚úÖ **Robust Statistical Testing:**  
   - Pearson's r calculation  
   - Two-tailed significance testing  

‚úÖ **Practical Interpretation:**  
   - Business-focused conclusions  
   - Clear effect size reporting  

### üîç Why This Matters  
‚Ä¢ Demonstrates proper **bivariate correlation** analysis  
‚Ä¢ Highlights importance of **testing assumptions**  
‚Ä¢ Provides framework for **marketing ROI evaluation**  

### üí° Business Value  
üìå **For CFOs:**  
   - Evidence for budget allocation decisions  
   - Identifies weak spending-revenue linkages  

üìå **For Marketing Teams:**  
   - Guides campaign optimization strategies  
   - Supports case for testing alternative channels  

üìå **For Data Teams:**  
   - Complete correlation analysis template  
   - Assumption validation best practices  

### üìà Statistical Approach  
1. **Assumption Verification:**  
   - Normality (Shapiro-Wilk p = 0.6552 & 0.2083)  
   - Homogeneity (Levene's p < 0.001)  

2. **Pearson Correlation Results:**  
   - r = 0.0545 (Very weak positive)  
   - p-value = 0.5904  
   - 95% CI: [-0.14, 0.24]  

### üîë Key Findings  
‚Ä¢ **Statistically Insignificant** relationship (p > 0.05)  
‚Ä¢ **Negligible Effect Size** (r < 0.1)  
‚Ä¢ Only **5.4% of revenue variance** explained by ad spend  

### üì¢ Strategic Conclusion  

**No significant linear relationship exists between advertising spend and revenue (p = 0.59). Businesses should:**  
1Ô∏è‚É£ **Investigate other revenue drivers**  
2Ô∏è‚É£ **Optimize existing campaigns before increasing budgets**  
3Ô∏è‚É£ **Test alternative marketing channels**


---

### 7. Mann-Whitney U Test

üìÑ **Project Link:** **[Mann-Whitney U Test.ipynb](Hypothesis%20Testing/Hypothesis%20Tests/5.%20Mann-Whitney%20U%20Test.ipynb)**

## üìä **Non-Parametric Comparison: North vs South Sales Performance**  

### üåü Overview  
This project employs the **Mann-Whitney U Test** to compare sales revenue distributions between two retail regions without normality assumptions. Using synthetic gamma-distributed data, we demonstrate proper non-parametric analysis workflow from assumption checks to business interpretation.

### ‚ùì Problem Statement  
"Is there a statistically significant difference in sales performance between our North and South regions, given non-normal revenue distributions?"  

### üéØ Objective  
‚Ä¢ Compare regional sales distributions robustly  
‚Ä¢ Demonstrate non-parametric testing methodology  
‚Ä¢ Guide regional strategy decisions with data  

### üìä Dataset Description  
**Synthetic Revenue Data (200 Records):**  
‚Ä¢ **North Region (100 stores):**  
  - Gamma distribution (shape=2, scale=500)  
  - Positively skewed  
‚Ä¢ **South Region (100 stores):**  
  - Gamma distribution (shape=2, scale=520)  
  - Slightly higher potential values  

**Variables:**  
‚Ä¢ `region`: Categorical (North/South)  
‚Ä¢ `revenue`: Continuous numerical values  

### üöÄ Project Highlights  
‚úÖ **Proper Assumption Handling:**  
   - Confirmed non-normality (Shapiro-Wilk p < 0.001)  
   - Verified equal variances (Levene's p = 0.198)  

‚úÖ **Robust Statistical Testing:**  
   - Correct application of Mann-Whitney U test  
   - Clear interpretation of rank-based comparison  

‚úÖ **Actionable Business Insights:**  
   - Translates statistical results to strategic guidance  
   - Provides framework for regional performance analysis  

### üîç Why This Matters  
‚Ä¢ Real-world demonstration of **non-parametric testing**  
‚Ä¢ Shows alternative when t-test assumptions fail  
‚Ä¢ Provides template for **skewed data comparisons**  

### üí° Business Value  
üìå **For Regional Managers:**  
   - Evidence-based performance evaluation  
   - Avoids misleading normal-distribution assumptions  

üìå **For Data Teams:**  
   - Complete non-parametric analysis example  
   - Proper assumption validation techniques  

üìå **For Executives:**  
   - Confidence in regional comparisons  
   - Guides resource allocation decisions  

### üìà Statistical Approach  
1. **Assumption Verification:**  
   - Shapiro-Wilk: Confirmed non-normality (p < 0.001)  
   - Levene's Test: Equal variances (p = 0.198)  

2. **Mann-Whitney U Test Results:**  
   - U-statistic: 4549.0  
   - p-value: 0.2710  
   - Effect Size: Rank-biserial correlation optional  

### üîë Key Findings  
‚Ä¢ **High p-value (0.271 > 0.05)** ‚Üí Fail to reject null  
‚Ä¢ **No significant difference** in revenue distributions  
‚Ä¢ Median revenues:  
  - North: $963.42  
  - South: $996.87  

### üì¢ Strategic Conclusion  

**No statistically significant difference exists between North and South sales distributions (p = 0.27). Regional strategies should:** 

1Ô∏è‚É£ **Focus on non-geographic performance drivers**
2Ô∏è‚É£ **Investigate higher-level business factors**  
3Ô∏è‚É£ **Maintain consistent policies across regions**


---

## 8. Wilcoxon Signed-Rank Test

üìÑ **Project Link:** **[Wilcoxon Signed-Rank Test.ipynb](Hypothesis%20Testing/Hypothesis%20Tests/6.%20Wilcoxon%20Signed-Rank%20Test.ipynb)**

## üìà Wilcoxon Signed-Rank Test: Promotional Campaign Impact Analysis  

### üåü Overview  
This project applies the **Wilcoxon Signed-Rank Test** to evaluate whether a promotional campaign significantly increased daily sales. Using synthetic paired data that violates normality assumptions, we demonstrate proper non-parametric analysis for before-after studies.

### ‚ùì Problem Statement  
"Did our recent promotional campaign actually increase sales, given that our sales data isn't normally distributed?"  

### üéØ Objective  
‚Ä¢ Quantify campaign impact without normality assumptions  
‚Ä¢ Demonstrate robust paired data analysis  
‚Ä¢ Provide actionable marketing insights  

### üìä Dataset Description  
**Synthetic Sales Data (60 Days):**  
‚Ä¢ **Before Campaign:**  
  - Gamma distribution (shape=2, scale=100)  
  - Median: $191.42  
‚Ä¢ **After Campaign:**  
  - Before sales + N(10,15) boost  
  - Median: $200.87 (+4.9%)  

**Variables:**  
‚Ä¢ `before_sales`: Daily sales pre-campaign  
‚Ä¢ `after_sales`: Daily sales post-campaign  

### üöÄ Project Highlights  
‚úÖ **Assumption Validation:**  
   - Confirmed non-normality of differences (Shapiro-Wilk p=0.295)  
   - Proper handling of paired observations  

‚úÖ **Robust Statistical Testing:**  
   - Correct application of Wilcoxon signed-rank test  
   - Effect size calculation (optional)  

‚úÖ **Business-Ready Insights:**  
   - Clear interpretation of median differences  
   - Statistical significance with practical implications  

### üîç Why This Matters  
‚Ä¢ Real-world demonstration of **non-parametric paired testing**  
‚Ä¢ Shows alternative when paired t-test assumptions fail  
‚Ä¢ Provides template for **marketing intervention analysis**  

### üí° Business Value  
üìå **For Marketing Teams:**  
   - Proof of campaign effectiveness  
   - Justification for future promotions  

üìå **For Store Managers:**  
   - Quantified sales impact  
   - Benchmark for future initiatives  

üìå **For Data Teams:**  
   - Complete non-parametric workflow  
   - Best practices for skewed paired data  

### üìà Statistical Approach  
1. **Assumption Verification:**  
   - Shapiro-Wilk on differences (p=0.295 ‚Üí non-normal)  

2. **Wilcoxon Signed-Rank Results:**  
   - Test Statistic: 309.0  
   - p-value: <0.001  
   - Median Increase: +$9.45 (4.9%)  

### üîë Key Findings  
‚Ä¢ **Highly Significant Result** (p < 0.0001)  
‚Ä¢ **Median Sales Increase**: +4.9%  
‚Ä¢ **Practical Impact**: Campaign successfully boosted sales  

### üì¢ Executive Conclusion  
**The promotional campaign **significantly increased daily sales** by a median of 4.9% (p < 0.001). This strong evidence supports:**  
1Ô∏è‚É£ **Repeating similar campaigns**  
2Ô∏è‚É£ **Allocating budget for promotions**  
3Ô∏è‚É£ **Further testing to optimize impact**

---

## 9. Kruskal-Wallis Test

üìÑ **Project Link:** **[Kruskal-Wallis Test](Hypothesis%20Testing/Hypothesis%20Tests/7.%20Kruskal-Wallis%20Test.ipynb)**

## üìä **Kruskal-Wallis Test: Comparing Marketing Channel Performance**  

### üåü Overview  
This project applies the **Kruskal-Wallis H-test** to evaluate whether sales performance differs across three marketing channels (Email, Social Media, TV) when data violates normality assumptions. The analysis provides robust insights for marketing budget allocation decisions.

### ‚ùì Problem Statement  
"Do our marketing channels (Email vs Social Media vs TV) generate significantly different sales results, given non-normal revenue distributions?"  

### üéØ Objective  
‚Ä¢ Compare channel performance without normality assumptions  
‚Ä¢ Demonstrate proper non-parametric multi-group analysis  
‚Ä¢ Guide marketing budget allocation decisions  

### üìä Dataset Description  
**Synthetic Sales Data (150 Records):**  
‚Ä¢ **Email Campaign (50):**  
  - Gamma distribution (shape=2, scale=120)  
  - Median: $229.18  
‚Ä¢ **Social Media (50):**  
  - Gamma distribution (shape=2, scale=110)  
  - Median: $210.65  
‚Ä¢ **TV Campaign (50):**  
  - Gamma distribution (shape=2, scale=140)  
  - Median: $268.94  

**Variables:**  
‚Ä¢ `channel`: Categorical (Email/Social/TV)  
‚Ä¢ `sales`: Continuous numerical values  

### üöÄ Project Highlights  
‚úÖ **Comprehensive Assumption Checks:**  
   - Confirmed non-normality for all channels (Shapiro-Wilk p < 0.05)  
   - Verified ordinal data structure  

‚úÖ **Robust Statistical Testing:**  
   - Correct application of Kruskal-Wallis test  
   - Post-hoc analysis available if significant  

‚úÖ **Actionable Business Insights:**  
   - Clear interpretation of median sales  
   - Statistical significance with practical implications  

### üîç Why This Matters  
‚Ä¢ Real-world demonstration of **non-parametric ANOVA alternative**  
‚Ä¢ Shows proper handling of **skewed multi-group data**  
‚Ä¢ Provides template for **marketing channel evaluation**  

### üí° Business Value  
üìå **For CMOs:**  
   - Data-driven channel performance assessment  
   - Objective budget allocation guidance  

üìå **For Marketing Teams:**  
   - Identifies equally effective channels  
   - Prevents over-investment in underperforming channels  

üìå **For Data Teams:**  
   - Complete non-parametric workflow  
   - Best practices for ordinal data analysis  

### üìà Statistical Approach  
1. **Assumption Verification:**  
   - Shapiro-Wilk tests (All p < 0.05 ‚Üí non-normal)  

2. **Kruskal-Wallis Test Results:**  
   - H-statistic: 2.6024  
   - p-value: 0.2722  
   - Group Medians:  
     ‚Ä¢ Email: $229.18  
     ‚Ä¢ Social: $210.65  
     ‚Ä¢ TV: $268.94  

### üîë Key Findings  
‚Ä¢ **High p-value (0.272 > 0.05)** ‚Üí Fail to reject null  
‚Ä¢ **No significant difference** in sales distributions  
‚Ä¢ Observed median differences likely due to random variation  

### üì¢ Strategic Conclusion  
**All three marketing channels perform **statistically similarly** in sales generation (p = 0.27). Recommendations:**  
1Ô∏è‚É£ **Allocate budgets based on cost-efficiency rather than sales differences**  
2Ô∏è‚É£ **Investigate other performance metrics (ROI, customer acquisition cost)**  
3Ô∏è‚É£ **Consider testing additional channels or campaign variations**

---

## 10. Chi-Square Goodness of Fit Test

üìÑ **Project Link:** **[Chi-Square Goodness.ipynb
](Hypothesis%20Testing/Hypothesis%20Tests/8.%20Chi-Square%20Goodness.ipynb)**

üìä Chi-Square Goodness of Fit: Marketing Campaign Analysis  

### üåü Overview  
This project applies the **Chi-Square Goodness of Fit Test** to evaluate whether observed customer responses across three marketing campaigns match expected historical patterns. The analysis helps validate marketing channel performance assumptions.

### ‚ùì Problem Statement  
"Does our current customer response distribution across Email, Social Media, and TV campaigns differ significantly from what we historically expect?"  

### üéØ Objective  
‚Ä¢ Test alignment between observed and expected customer distributions  
‚Ä¢ Demonstrate proper Chi-Square Goodness of Fit implementation  
‚Ä¢ Provide data-driven insights for campaign evaluation  

### üìä Dataset Description  
**Synthetic Customer Response Data:**  
‚Ä¢ **Observed Counts:**  
  - Email: 120 customers  
  - Social Media: 95 customers  
  - TV: 85 customers  
‚Ä¢ **Expected Proportions:**  
  - Email: 40%  
  - Social Media: 35%  
  - TV: 25%  

**Total Customers:** 300  

### üöÄ Project Highlights  
‚úÖ **Proper Test Selection:**  
   - Designed for categorical frequency data  
   - Non-parametric (no normality assumptions)  

‚úÖ **Rigorous Assumption Checks:**  
   - Verified expected frequencies ‚â•5  
   - Confirmed independence of observations  

‚úÖ **Clear Interpretation:**  
   - Business-focused conclusions  
   - Practical significance assessment  

### üîç Why This Matters  
‚Ä¢ Demonstrates **distribution validation** technique  
‚Ä¢ Provides framework for **marketing performance benchmarking**  
‚Ä¢ Shows proper **categorical data analysis** approach  

### üí° Business Value  
üìå **For Marketing Teams:**  
   - Validates campaign performance assumptions  
   - Guides resource allocation decisions  

üìå **For Data Analysts:**  
   - Complete Chi-Square workflow example  
   - Expected frequency calculation method  

üìå **For Executives:**  
   - Confidence in campaign distribution patterns  
   - Evidence for strategic planning  

### üìà Statistical Approach  
1. **Assumption Verification:**  
   - All expected counts ‚â•5 (Met)  
   - Independent observations (By design)  

2. **Chi-Square Test Results:**  
   - œá¬≤ Statistic: 2.2857  
   - p-value: 0.3189  
   - Degrees of Freedom: 2  

### üîë Key Findings  
‚Ä¢ **High p-value (0.319 > 0.05)** ‚Üí Fail to reject null  
‚Ä¢ **No significant difference** between observed/expected  
‚Ä¢ Current campaign responses match historical patterns  

### üì¢ Strategic Conclusion  
**Customer responses align with historical distributions across all channels (p = 0.32). Recommended actions:**  
1Ô∏è‚É£ **Maintain current campaign mix**  
2Ô∏è‚É£ **Monitor for future deviations**  
3Ô∏è‚É£ **Investigate absolute performance metrics**

---

### üõ†Ô∏è Technical Implementation  
   - Python (Pandas, NumPy)  
   - SciPy (kruskal, shapiro)  
   - Matplotlib/Seaborn (Visualizations)


---

## How to Run These Projects

1. Open the project in [Google Colab](https://colab.research.google.com/)
2. Upload your Excel dataset when prompted.
3. Run all cells in order:
   - Data loading
   - EDA
   - Contingency table creation
   - Visualization
   - Statistical testing
4. Review the final statistical output and visualizations.

---

## Conclusion

This collection of projects provides a foundational understanding of diverse hypothesis testing methods applicable in business contexts. By rigorously checking assumptions and applying appropriate parametric or non-parametric tests, these projects showcase how statistical testing can validate business hypotheses, support decision-making, and uncover actionable insights.

---

